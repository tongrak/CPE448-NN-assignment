{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate\n",
    "variable(value, list, and matrix) and function uses in neural network training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Variable\n",
    "global eta;\n",
    "eta = 0.9;\n",
    "# External Input based list\n",
    "global xList;\n",
    "xList = []\n",
    "global dList;\n",
    "dList = []\n",
    "# Internal lists and matrix\n",
    "global eList\n",
    "eList = []\n",
    "global biasList\n",
    "biasList = [];\n",
    "global wMatrix\n",
    "wMatrix = [];\n",
    "global vMatrix\n",
    "vMatrix = []\n",
    "global yMatrix\n",
    "yMatrix = [];\n",
    "global sigmaMatrix\n",
    "sigmaMatrix = [];\n",
    "global nodeIndexMatrix\n",
    "nodeIndexMatrix = [];\n",
    "# Class order. use in determine class encoded form\n",
    "classOrder = [\"cp\",\"im\",\"pp\",\"imU\",\"om\",\"omL\",\"imL\",\"imS\"];"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Following are the functions use during forward and backware propagation of the neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeLayer(nodeNum:int):\n",
    "    # Return layer number of given node number\n",
    "    # Parameter: interger representing node number\n",
    "    # Raise Runtime error if given node does existed\n",
    "    result = 0;\n",
    "    for layer in nodeIndexMatrix:\n",
    "        if nodeNum in layer: return result\n",
    "        result += 1\n",
    "    raise RuntimeError(\"no node \" + nodeNum + \"existed in nodeIdexMatrix\");\n",
    "\n",
    "def getNodeLayerNIndex(nodeNum:int):\n",
    "    #Return layer number and index in said layer of given node number\n",
    "    # Parameter: interger representing node number\n",
    "    # Raise Runtime error if given node does existed\n",
    "    nodeLayer = getNodeLayer(nodeNum)\n",
    "    nodeIndex = nodeIndexMatrix[nodeLayer].index(nodeNum)\n",
    "    return nodeLayer, nodeIndex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation Function\n",
    "All following functions implemented using coordination-like node system and matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vFunc(nodeNum:int):\n",
    "    # Return v value of given nodeNum.\n",
    "    # Parameter: interger representing node number\n",
    "    nodeLayer, nodeIndex = getNodeLayerNIndex(nodeNum);\n",
    "    if nodeLayer == 0:\n",
    "        return 0;\n",
    "    else:\n",
    "        pastIndexes = nodeIndexMatrix[nodeLayer-1];\n",
    "        pastWs = list(map(lambda x: wMatrix[x][nodeIndex], pastIndexes)) # get all left layer's weight associate with given nodeNum\n",
    "        pastYs = yMatrix[nodeLayer-1] # get all left layer's y \n",
    "        wNyList = list(map(lambda x,y: x*y, pastYs,pastWs)) # calculate a sum of product of weight and its y.\n",
    "        bias = biasList[nodeNum]; #get given node bias\n",
    "        v = sum(wNyList)+bias; #calculate v\n",
    "        return v;\n",
    "\n",
    "def simgFunc(input:int):\n",
    "    # Return result of given input's sigmoid activation function\n",
    "    # Parameter: integer\n",
    "    return 1/(1 + np.exp(-input));\n",
    "\n",
    "def simgPrimeFunc(input:int):\n",
    "    # Return result of given input's derivatived sigmoid activation calculation \n",
    "    # Parameter: integer\n",
    "    return simgFunc(input)*(1-simgFunc(input));\n",
    "\n",
    "def yFunc(nodeNum:int):\n",
    "    # Return y value of given nodeNum (based on its v value).\n",
    "    # Parameter: interger representing node number\n",
    "    nodeLayer, nodeIndex = getNodeLayerNIndex(nodeNum);\n",
    "    # print(\"y index of:\" + str(nodeNum))\n",
    "    if nodeLayer == 0: # check if given node is input node\n",
    "        return xList[nodeNum];\n",
    "    else:\n",
    "        v = vMatrix[nodeLayer][nodeIndex]; # get its calculated v values\n",
    "        return simgFunc(v); # calculate and return sigmoid-ed v\n",
    "\n",
    "def eFunc(nodeNum:int):\n",
    "    # Return error of given nodeNum\n",
    "    # Parameter: interger representing node number\n",
    "    nodeLayer, nodeIndex = getNodeLayerNIndex(nodeNum);\n",
    "    if nodeLayer == len(nodeIndexMatrix)-1: # check if given node is output node\n",
    "        y = yMatrix[nodeLayer][nodeIndex] #get its y value\n",
    "        d = dList[nodeIndex] # get its desire value\n",
    "        return d - y;\n",
    "    else:\n",
    "        raise RuntimeError(\"Can't calculate e(\"+str(nodeNum)+\")\")\n",
    "\n",
    "def sigmaFunc(nodeNum:int):\n",
    "    # Return sigma or (E) of given nodeNum\n",
    "    # Parameter: interger representing node number\n",
    "    nodeLayer, nodeIndex = getNodeLayerNIndex(nodeNum);\n",
    "    phiPrime = simgPrimeFunc(vFunc(nodeNum))\n",
    "    if nodeLayer == len(nodeIndexMatrix)-1: # check if given node is output node\n",
    "        e = eFunc(nodeNum);\n",
    "        return e*phiPrime;\n",
    "    else:\n",
    "        forWs = wMatrix[nodeNum]; # Get all its weigh with edge pointing to the right\n",
    "        forSigmas = sigmaMatrix[nodeLayer+1] # get all sigma (E) values of right layer \n",
    "        sumSigW = sum(map(lambda x,y: x*y,forWs, forSigmas)); # calculate product of all weight and its pointed sigma\n",
    "        return phiPrime*sumSigW; # calculate and return sigma \n",
    "\n",
    "def deltaWFunc(nodeI:int, nodeJ:int):\n",
    "    # Return delta W of node I and J\n",
    "    # Parameter: interger representing node I and J number\n",
    "    nodeILayer, nodeIIndex = getNodeLayerNIndex(nodeI);\n",
    "    nodeJLayer, nodeJIndex = getNodeLayerNIndex(nodeJ);\n",
    "    yI = yMatrix[nodeILayer][nodeIIndex]; # get node I's y values\n",
    "    if nodeJLayer == len(nodeIndexMatrix)-1: # Check if given node is output node\n",
    "        e = eFunc(nodeJ);\n",
    "        phiPrime = simgPrimeFunc(vFunc(nodeJ))\n",
    "        return eta*e*phiPrime*yI;\n",
    "    else:\n",
    "        sigmaJ = sigmaMatrix[nodeJLayer][nodeJIndex];\n",
    "        return eta*sigmaJ*yI;\n",
    "\n",
    "def getNewWs(nodeNum:int):\n",
    "    # Return new weight based on delta w of given nodeNUm\n",
    "    # Parameter: interger representing node number\n",
    "    nodeLayer, nodeIndex = getNodeLayerNIndex(nodeNum);\n",
    "    if nodeLayer == len(nodeIndexMatrix)-1: # Check if given node is output node\n",
    "        raise RuntimeError(\"Cann't calculate delta-Ws of output node:\" + str(nodeNum))\n",
    "    forNodeIndexes = nodeIndexMatrix[nodeLayer+1]; # get all node number of right layer\n",
    "    oldWs = wMatrix[nodeNum] # get its weights associate with right layer\n",
    "    deltaWs = list(map(lambda x: deltaWFunc(nodeNum, x), forNodeIndexes)) # get all delta w of said weights (oldWs)\n",
    "    newWs = list(map(lambda x,y: x+y, oldWs,deltaWs)) # calculate all new weights\n",
    "    return newWs;\n",
    "\n",
    "def getNewBias(nodeNum:int):\n",
    "    # Return new bias of given nodeNUm\n",
    "    # Parameter: interger representing node number\n",
    "    nodeLayer, nodeIndex = getNodeLayerNIndex(nodeNum);\n",
    "    if nodeLayer == 0: raise RuntimeError(\"No bias for node:\"+str(nodeNum))  # if given node is input node raise runtime err\n",
    "    sigma = sigmaMatrix[nodeLayer][nodeIndex]; # get sigma values of given node\n",
    "    oldBias = biasList[nodeNum]; # get current bias of given node\n",
    "    newBias = oldBias+(eta*sigma); # calculate new bias based on old bias\n",
    "    return newBias;\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Section\n",
    "Following functions design to simplify the neural network experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "Functions uses as parts of trains and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def initAllContainer(layerNodeCount:list):\n",
    "    # Clear and create all container use in the neural network\n",
    "    # Parameter: a list of integer representing node count in each layer\n",
    "     \n",
    "    #Clear section\n",
    "    xList.clear();\n",
    "    dList.clear();\n",
    "    eList.clear();\n",
    "    biasList.clear();\n",
    "    wMatrix.clear();\n",
    "    vMatrix.clear()\n",
    "    yMatrix.clear();\n",
    "    sigmaMatrix.clear();\n",
    "    nodeIndexMatrix.clear();\n",
    "    #Container creation \n",
    "    currIndex = 0;\n",
    "    for l in layerNodeCount:\n",
    "        nodeIndexMatrix.append(list(range(currIndex,currIndex+l)))\n",
    "        yMatrix.append([0]*l)\n",
    "        vMatrix.append([0]*l)\n",
    "        sigmaMatrix.append([0]*l)\n",
    "        currIndex += l\n",
    "    for layer in range(len(nodeIndexMatrix)):\n",
    "        for node in nodeIndexMatrix[layer]:\n",
    "            if not node in nodeIndexMatrix[len(nodeIndexMatrix)-1]:\n",
    "                wMatrix.append([0]*len(nodeIndexMatrix[layer+1]))\n",
    "    for _ in range(max(max(nodeIndexMatrix))+1):\n",
    "        # eList.append(0);\n",
    "        biasList.append(0);\n",
    "\n",
    "def getRandomValue(minVal:float, maxVal:float):\n",
    "    # Return a random any values but zero in given range\n",
    "    # Parameters: minimal and maximal float values \n",
    "    result = 0\n",
    "    while result == 0:\n",
    "        result = round(random.uniform(minVal, maxVal),2)\n",
    "    return result;\n",
    "\n",
    "def UpdateRandomWMatrix(minVal:float, maxVal:float):\n",
    "    # Update existed W Matrix with random value in given range\n",
    "    # Parameters: minimal and maximal float values \n",
    "    for i in range(len(wMatrix)):\n",
    "        for j in range(len(wMatrix[i])):\n",
    "            wMatrix[i][j] = getRandomValue(minVal, maxVal)            \n",
    "\n",
    "def UndateRandomBias(minVal:float, maxVal:float):\n",
    "    # Update existed Bias list with random value in given range\n",
    "    # Parameters: minimal and maximal float values \n",
    "    for i in range(len(biasList)):\n",
    "        biasList[i] = getRandomValue(minVal, maxVal)  \n",
    "\n",
    "def getXnDList(textsList:list):\n",
    "    # Return a list of input and encode class\n",
    "    # Parameter: a String representing tuple of data\n",
    "    dataText = textsList;\n",
    "    dataList = dataText.split();\n",
    "    clas = dataList.pop(7)\n",
    "    xL = list(map(float,dataList))\n",
    "    dL = [0]*8;\n",
    "    dL[classOrder.index(clas)] = 1;\n",
    "    return xL, dL;\n",
    "\n",
    "def forwardPropergate():\n",
    "    # Update all v Matrix and y Matrix values in forward propagation manner\n",
    "    for layNum in range(len(nodeIndexMatrix)):\n",
    "        nodelist = nodeIndexMatrix[layNum]\n",
    "        vList = list(map(vFunc, nodelist));\n",
    "        vMatrix[layNum] = vList\n",
    "        yList = list(map(yFunc, nodelist));\n",
    "        yMatrix[layNum] = yList\n",
    "\n",
    "def backPropergate():\n",
    "    # Update all sigma Matrix, w Matrix, and bias list in back propagation manner\n",
    "    reverseLayNum = list(range(len(nodeIndexMatrix))).copy()\n",
    "    reverseLayNum.pop(0)\n",
    "    reverseLayNum.reverse() # Create a list contain all node number in reverse manner\n",
    "    # Update all sigma\n",
    "    for layNum in reverseLayNum:\n",
    "        nodelist = nodeIndexMatrix[layNum]\n",
    "        sigmaList = list(map(sigmaFunc, nodelist));\n",
    "        sigmaMatrix[layNum] = sigmaList\n",
    "    # Update all W\n",
    "    indexesForW = nodeIndexMatrix.copy()\n",
    "    indexesForW.pop(len(nodeIndexMatrix)-1)\n",
    "    for node in range(max(max(indexesForW))): # looping from least to most valued node number\n",
    "        newWs = getNewWs(node);\n",
    "        wMatrix[node] = newWs;\n",
    "    # Update all Biase\n",
    "    startBiasIndex = max(nodeIndexMatrix[0])\n",
    "    for b in range(startBiasIndex+1,max(max(nodeIndexMatrix))+1): # Looping through hidden and output node\n",
    "        newBias = getNewBias(b);\n",
    "        biasList[b] = newBias;\n",
    "\n",
    "def getResult(yList:list):\n",
    "    # Return a predicted result based on given y / confident list\n",
    "    # Parameters: list representing y list of output nodes\n",
    "    result = [0]*8\n",
    "    maxIndex = yList.index(max(yList));\n",
    "    result[maxIndex] = 1\n",
    "    return result\n",
    "\n",
    "def grouper(inputList:list, n:int):\n",
    "    # Return a list of size n group of given inputlist\n",
    "    # Parameters:\n",
    "    #  InputList: list to be regrouping\n",
    "    #  n: integer representing size of most group\n",
    "    args = [iter(inputList)] * n\n",
    "    return list([e for e in t if e != None] for t in itertools.zip_longest(*args))\n",
    "\n",
    "def getTrainNTestIndexesHoldUp(rowCount:int):\n",
    "    # Return train and test data's indexes determining based on Hold-up method (randomly pick 90% of all indexes to be uses at train)\n",
    "    allIndexes = range(rowCount);\n",
    "    trainDataIndexes = random.sample(allIndexes, round(rowCount*0.9))\n",
    "    testDataIndexes = [x for x in allIndexes if x not in trainDataIndexes]\n",
    "    return trainDataIndexes, testDataIndexes\n",
    "\n",
    "def getTrainNTestIndexes(rowCount:int, kNumber:int, kRound:int):\n",
    "    # Return trian and test data's indexes of kRound fold.\n",
    "    # Parameters:\n",
    "    #  rowCount: integer representing total number of data instance\n",
    "    #  kNumber: integer representing k in k-fold cross validation method\n",
    "    #  kRound: integer representing k-fold of current validation\n",
    "    indexesMatr = grouper(range(rowCount), round(rowCount/kNumber));\n",
    "    testDataIndexes = indexesMatr[kRound]\n",
    "    trainDataIndexes = [x for subList in indexesMatr for x in subList]\n",
    "    trainDataIndexes = [x for x in trainDataIndexes if x not in testDataIndexes]\n",
    "    return trainDataIndexes, testDataIndexes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNNResult(dataPath:str, NNDimen:list, learningRate:float, epoch:int, kNumber:int):\n",
    "    # Return a list of accuracies percentage and losses of each fold test. Said tests are a part of neural network create based on given parameter.\n",
    "    # Parameters:\n",
    "    #   dataPath: String representing path to preprocessed data\n",
    "    #   NNDimen: list of integer representing node count in each layer\n",
    "    #  learningRate: float representing eta to be uses.\n",
    "    #  epoch: integer representing how many each train dataset to be pass through neural network\n",
    "\n",
    "    # init variable assoviate with dataset\n",
    "    fil = open(dataPath, \"r\")\n",
    "    texts = fil.readlines()\n",
    "    random.shuffle(texts)\n",
    "    rowCount = len(texts);\n",
    "\n",
    "    # init variable to be use in neural network\n",
    "    global eta;\n",
    "    eta = learningRate;\n",
    "    initAllContainer(NNDimen);\n",
    "    UpdateRandomWMatrix(-0.1,0.1);\n",
    "    UndateRandomBias(-0.1,0.1);\n",
    "    lossList = []\n",
    "    accList = []\n",
    "\n",
    "    # Begin k - fold of train and test\n",
    "    for k in range(kNumber):\n",
    "        trainIndexes, testIndexes = getTrainNTestIndexes(rowCount, kNumber, k);\n",
    "        # Train section\n",
    "        for _ in range(epoch):\n",
    "            global xList\n",
    "            global dList\n",
    "            for i in trainIndexes:\n",
    "                xList, dList = getXnDList(texts[i]); # determining input and encode class\n",
    "                forwardPropergate();\n",
    "                backPropergate();\n",
    "        \n",
    "        # Test section\n",
    "        corrCount = 0;\n",
    "        totaLoss = 0\n",
    "        for i in testIndexes:\n",
    "            xList, dList = getXnDList(texts[i]); # determining input and encode class\n",
    "            forwardPropergate();\n",
    "            rawY = yMatrix[len(nodeIndexMatrix)-1] # get output layer y (confidence) values\n",
    "            loss = sum(map(lambda x,y: x*math.log(y),dList, rawY)); # calculate loss based on predicted and actually result\n",
    "            result = getResult(rawY);  # get the predicted result\n",
    "            if (dList==result): corrCount += 1; # correct counting\n",
    "            totaLoss += loss;\n",
    "        lossList.append((-1/len(testIndexes))*totaLoss); # calculate and record loss of current fold\n",
    "        accList.append(corrCount/len(testIndexes)*100); # calculate and record accuracy of current fold\n",
    "    return accList, lossList;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Section\n",
    "A section for config all neural network parameter such as datapath, architecture, learning rate run begin the experiment. Follow by a plotting of line graph representing accuracy and loss of recorded experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultAccs, resultLosses = getNNResult(\"ProcessedData/type3data.data\", [7,8,8],0.01,50,10);\n",
    "# Accuracy Graph creation\n",
    "plt.title(\"Accuracy\")\n",
    "plt.ylabel(\"Accuracy(%)\")\n",
    "plt.xlabel(\"fold\")\n",
    "temp = list(map(lambda x: round(x,3), resultAccs))\n",
    "plt.plot(temp)\n",
    "plt.show()\n",
    "# Loss graph creation\n",
    "plt.title(\"Categorical Cross-Entropy loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"fold\")\n",
    "temp = list(map(lambda x: round(x,3), resultLosses))\n",
    "plt.plot(temp)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5d4d8f8c396911fa7ea230123ef05aeb4e9f2b4fb5da5dcce73e71fee68065d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
